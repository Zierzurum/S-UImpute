{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p82UX4ypvl6",
        "outputId": "35261aa5-d4df-4294-da75-5903e061c9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.75)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.17.0)\n",
            "Requirement already satisfied: pykalman in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (24.2)\n",
            "Requirement already satisfied: scikit-base<0.13.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (0.12.3)\n",
            "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 222, in iter_dependencies\n",
            "    req = Requirement(req_string.strip())\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
            "    marker = _parse_requirement_marker(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 151, in _parse_requirement_marker\n",
            "    marker = _parse_marker(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
            "    expression = [_parse_marker_atom(tokenizer)]\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
            "    marker = _parse_marker_item(tokenizer)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
            "    marker_var_right = _parse_marker_var(tokenizer)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 317, in _parse_marker_var\n",
            "    return process_python_str(tokenizer.read().text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 332, in process_python_str\n",
            "    value = ast.literal_eval(python_str)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ast.py\", line 64, in literal_eval\n",
            "    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n",
            "    return compile(source, filename, mode, flags,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install pykalman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cRVna1opzHs",
        "outputId": "b4002ad6-4e2d-490b-e86b-6c7306c9ef70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ9XJt1yp1u2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from pykalman import KalmanFilter\n",
        "#from missingpy import MissForest\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPZzOcRqp4x6"
      },
      "outputs": [],
      "source": [
        "#dataset_path = '/content/drive/MyDrive/eksik_veri_simulasyonlari/MS_missing_with_existing_25pct.xlsx'\n",
        "dataset_path = '/content/drive/MyDrive/VNS_dataset/VNS_Tepebasi.xlsx'\n",
        "#dataset_path = '/content/drive/MyDrive/simulated_missing_xlsx/C.xlsx'\n",
        "\n",
        "df = pd.read_excel(dataset_path, parse_dates=['Tarih'])\n",
        "df.replace(\"-\", np.nan, inplace=True)\n",
        "df['Tarih'] = pd.to_datetime(df['Tarih'], dayfirst=True, errors='coerce')\n",
        "\n",
        "df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].apply(lambda x: np.nan if x < 0 else x)\n",
        "max_val = np.nanmax(df['PM10 ( µg/m3 )'])\n",
        "df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].clip(lower=0, upper=max_val)\n",
        "df_new = df.iloc[29530:]\n",
        "print(df_new.head())\n",
        "#series = df['PM 2.5 ( µg/m3 )'].copy()\n",
        "series = df['PM10 ( µg/m3 )']\n",
        "#resid_full = series\n",
        "#dates = df['Tarih'].copy()\n",
        "#series=series.iloc[35049:]\n",
        "#dates = dates.iloc[35049:]\n",
        "#print(dates.head())\n",
        "n_total = len(series)\n",
        "split_idx = int(0.8 * n_total)\n",
        "train_series = series[:split_idx]\n",
        "test_series = series[split_idx:]\n",
        "test_dates = dates[split_idx:]\n",
        "train_series = train_series.reset_index(drop=True)\n",
        "subsample_dates_train = dates[:split_idx].reset_index(drop=True)\n",
        "print(len(series), len(train_series), len(test_series))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY6xxrZ6O0sA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "summary_records = []\n",
        "prediction_dict = {}\n",
        "\n",
        "for run in range(1, 11):\n",
        "    print(f\"▶️ Iteration {run} başlıyor...\")\n",
        "    dataset_path = '/content/drive/MyDrive/eksik_veri_simulasyonlari/Tepebasi_missing_with_existing_10pct.xlsx'\n",
        "    #dataset_path = '/content/drive/MyDrive/VNS_dataset/VNS_Odunpazari.xlsx'\n",
        "    df = pd.read_excel(dataset_path, parse_dates=['Tarih'])\n",
        "    df.replace(\"-\", np.nan, inplace=True)\n",
        "    df = df.iloc[35048:]\n",
        "    print(df.head())\n",
        "    df['Tarih'] = pd.to_datetime(df['Tarih'], dayfirst=True, errors='coerce')\n",
        "    df['PM 2.5 ( µg/m3 )'] = df['PM 2.5 ( µg/m3 )'].apply(lambda x: np.nan if x < 0 else x)\n",
        "    df['PM 2.5 ( µg/m3 )'] = df['PM 2.5 ( µg/m3 )'].clip(lower=0, upper=np.nanmax(df['PM 2.5 ( µg/m3 )']))\n",
        "\n",
        "    series = df['PM 2.5 ( µg/m3 )']\n",
        "    dates = df['Tarih']\n",
        "    #series = series.iloc[18630:].reset_index(drop=True)\n",
        "    #dates = dates.iloc[18630:].reset_index(drop=True)\n",
        "    print(dates.head())\n",
        "    n_total = len(series)\n",
        "    original_nan_mask = series.isna().to_numpy()\n",
        "    split_idx = int(0.8 * n_total)\n",
        "    train_series = series[:split_idx].reset_index(drop=True)\n",
        "    test_series = series[split_idx:].reset_index(drop=True)\n",
        "\n",
        "    # === 1. IMPUTATION YÖNTEMLERİ ===\n",
        "    def modified_smape(y_true, y_pred):\n",
        "      return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "    def kalman_filter_imputation(train, test):\n",
        "        kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1,\n",
        "                          initial_state_covariance=1, transition_matrices=[1],\n",
        "                          observation_matrices=[1], transition_covariance=1,\n",
        "                          observation_covariance=1)\n",
        "        train_masked = np.ma.array(train, mask=np.isnan(train))\n",
        "        train_imputed, _ = kf.smooth(train_masked)\n",
        "\n",
        "        combined = np.concatenate([train_imputed.flatten(), test])\n",
        "        combined_masked = np.ma.array(combined, mask=np.isnan(combined))\n",
        "        combined_imputed, _ = kf.smooth(combined_masked)\n",
        "        test_imputed = combined_imputed[len(train):]\n",
        "        return train_imputed.flatten(), test_imputed.flatten()\n",
        "\n",
        "    def interpolation_imputation(train, test):\n",
        "        train_imputed = pd.Series(train).interpolate(limit_direction='both').to_numpy()\n",
        "        combined = np.concatenate([train_imputed, test])\n",
        "        combined_imputed = pd.Series(combined).interpolate(limit_direction='both').to_numpy()\n",
        "        return train_imputed, combined_imputed[len(train):]\n",
        "\n",
        "    def backward_fill_imputation(train, test):\n",
        "        return pd.Series(train).bfill().ffill().to_numpy(), pd.Series(test).bfill().ffill().to_numpy()\n",
        "\n",
        "    def svr_imputation_separate(train, test, n_lags=3):\n",
        "        def create_lagged_df(series):\n",
        "            df = pd.DataFrame({'y': series})\n",
        "            for i in range(1, n_lags + 1):\n",
        "                df[f'lag_{i}'] = df['y'].shift(i)\n",
        "            return df\n",
        "\n",
        "        feature_names = [f'lag_{i}' for i in range(1, n_lags + 1)]\n",
        "        train_df = create_lagged_df(pd.Series(train)).dropna()\n",
        "        X_train = train_df[feature_names].to_numpy()\n",
        "        y_train = train_df['y'].to_numpy()\n",
        "        svr = SVR().fit(X_train, y_train)\n",
        "\n",
        "        train_filled = np.copy(train)\n",
        "        for idx in np.where(np.isnan(train))[0]:\n",
        "            if idx >= n_lags and not np.isnan(train[idx - n_lags:idx]).any():\n",
        "                x_pred = train[idx - n_lags:idx][::-1].reshape(1, -1)\n",
        "                train_filled[idx] = svr.predict(x_pred)[0]\n",
        "        train_filled = pd.Series(train_filled).interpolate().bfill().ffill().to_numpy()\n",
        "\n",
        "        test_filled = np.copy(test)\n",
        "        for idx in np.where(np.isnan(test))[0]:\n",
        "            full_input = np.concatenate([train_filled[-n_lags:], test[:idx]])\n",
        "            if len(full_input) >= n_lags and not np.isnan(full_input[-n_lags:]).any():\n",
        "                x_pred = full_input[-n_lags:][::-1].reshape(1, -1)\n",
        "                test_filled[idx] = svr.predict(x_pred)[0]\n",
        "        test_filled = pd.Series(test_filled).interpolate().bfill().ffill().to_numpy()\n",
        "        return train_filled, test_filled\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    # === 2. TÜM YÖNTEMLERİ UYGULA ===\n",
        "    kalman_train, kalman_test = kalman_filter_imputation(train_series.to_numpy(), test_series.to_numpy())\n",
        "    linear_train, linear_test = interpolation_imputation(train_series, test_series)\n",
        "    bfill_train, bfill_test = backward_fill_imputation(train_series, test_series)\n",
        "    svr_train, svr_test = svr_imputation_separate(train_series.to_numpy(), test_series.to_numpy())\n",
        "\n",
        "    # === 3. STACKING VERİSİ (TÜM DOLU GÖZLEMLER) ===\n",
        "    non_nan_train_idx = np.where(~np.isnan(train_series))[0]\n",
        "\n",
        "    X_train_stack = np.column_stack([\n",
        "        kalman_train[non_nan_train_idx],\n",
        "        linear_train[non_nan_train_idx],\n",
        "        bfill_train[non_nan_train_idx],\n",
        "        svr_train[non_nan_train_idx]\n",
        "    ])\n",
        "    y_train_stack = train_series.to_numpy()[non_nan_train_idx]\n",
        "\n",
        "    nan_train_idx = np.where(np.isnan(train_series))[0]\n",
        "    nan_test_idx = np.where(np.isnan(test_series))[0]\n",
        "\n",
        "    X_test_stack = np.column_stack([\n",
        "        kalman_test[nan_test_idx],\n",
        "        linear_test[nan_test_idx],\n",
        "        bfill_test[nan_test_idx],\n",
        "        svr_test[nan_test_idx]\n",
        "    ])\n",
        "\n",
        "    # === 4. META MODEL: GRADIENT BOOSTING ===\n",
        "    if len(X_train_stack) >= 10:\n",
        "        meta_model = GradientBoostingRegressor()\n",
        "        meta_model.fit(X_train_stack, y_train_stack)\n",
        "\n",
        "        # Yalnızca eksik değerler doldurulacak\n",
        "        train_series.iloc[nan_train_idx] = meta_model.predict(\n",
        "            np.column_stack([\n",
        "                kalman_train[nan_train_idx],\n",
        "                linear_train[nan_train_idx],\n",
        "                bfill_train[nan_train_idx],\n",
        "                svr_train[nan_train_idx]\n",
        "            ])\n",
        "        )\n",
        "        test_series.iloc[nan_test_idx] = meta_model.predict(X_test_stack)\n",
        "    else:\n",
        "        print(\"⚠️ Yetersiz eğitim verisi. Kalman ile dolduruluyor.\")\n",
        "        train_series.iloc[nan_train_idx] = kalman_train[nan_train_idx]\n",
        "        test_series.iloc[nan_test_idx] = kalman_test[nan_test_idx]\n",
        "\n",
        "    filled_series = pd.concat([train_series, test_series], ignore_index=True)\n",
        "    end_time = time.time()\n",
        "    imputation_time = end_time - start_time\n",
        "    print(f\"Imputation Time: {imputation_time} seconds\")\n",
        "\n",
        "    # === 5. GÖRSELLEŞTİRME ===\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(dates, series, label='Orijinal Seri (Eksik)', alpha=0.6)\n",
        "    plt.plot(dates, filled_series, label='Stacking + GB Regressor ile Doldurulmuş Seri')\n",
        "    plt.xlabel('Tarih')\n",
        "    plt.ylabel('PM 2.5 ( µg/m3 )')\n",
        "    plt.title('Eksik Veri Doldurma: Stacking ve GB Regressor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Eksik değer sayısı (doldurma sonrası):\", filled_series.isnull().sum())\n",
        "    filled_only = filled_series\n",
        "    #filled_only = final_filled_series\n",
        "    print(len(filled_only))\n",
        "\n",
        "    # === 1. Train / Test böl\n",
        "    split_ratio = 0.8\n",
        "    split_idx = int(len(filled_only) * split_ratio)\n",
        "    train_series = filled_only[:split_idx]\n",
        "    test_series = filled_only[split_idx:]\n",
        "    print(len(train_series))\n",
        "    print(len(test_series))\n",
        "    # === 2. Normalizasyon\n",
        "    scaler = MinMaxScaler()\n",
        "    train_scaled = scaler.fit_transform(train_series.to_numpy().reshape(-1, 1))\n",
        "    test_scaled = scaler.transform(test_series.to_numpy().reshape(-1, 1))\n",
        "\n",
        "    # === 3. Sliding window veri hazırlığı\n",
        "    def create_lstm_dataset(series, look_back=24):\n",
        "        X, y = [], []\n",
        "        for i in range(len(series) - look_back):\n",
        "            X.append(series[i:i+look_back])\n",
        "            y.append(series[i+look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    look_back = 24\n",
        "    X_train, y_train = create_lstm_dataset(train_scaled, look_back)\n",
        "    X_test, y_test = create_lstm_dataset(test_scaled, look_back)\n",
        "\n",
        "    # LSTM input boyutu: (samples, timesteps, features)\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    train_time_start = time.time()\n",
        "    # === 4. LSTM Modeli\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(30, input_shape=(look_back, 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.summary()\n",
        "\n",
        "    # === 5. Model eğitimi\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=50, verbose=1)\n",
        "    train_time_end = time.time()\n",
        "    train_time = train_time_end - train_time_start\n",
        "    print(f\"Training Time: {train_time} seconds\")\n",
        "\n",
        "    # === 6. Tahmin ve ters dönüşüm\n",
        "    train_pred = model.predict(X_train)\n",
        "\n",
        "    test_time_start = time.time()\n",
        "    test_pred = model.predict(X_test)\n",
        "    test_time_end = time.time()\n",
        "\n",
        "    test_time = test_time_end - test_time_start\n",
        "    print(f\"Test Time: {test_time} seconds\")\n",
        "\n",
        "    trainPredict = scaler.inverse_transform(train_pred)\n",
        "    trainY = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "    testPredict = scaler.inverse_transform(test_pred)\n",
        "    testY = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    print(len(testY), len(testPredict))\n",
        "\n",
        "    # Orijinal test verisi (eksik değer içeren)\n",
        "    #orig_train = alldata[0:train_size]  # Test kısmının orijinali\n",
        "    #orig_mask_train = ~np.isnan(orig_train)  # Eksik olmayanlar için maske\n",
        "    #valid_mask_train = orig_mask_train[look_back+1:]  # look_back sonrası geçerli indeksler\n",
        "\n",
        "\n",
        "    orig_train = series[0:split_idx]\n",
        "    orig_mask_train = ~np.isnan(orig_train)\n",
        "    valid_mask_train = orig_mask_train[look_back:look_back + len(trainY)]\n",
        "\n",
        "    print(f\"valid_mask_train: {valid_mask_train.shape}\")\n",
        "    print(f\"trainY: {trainY.shape}, trainPredict: {trainPredict.shape}\")\n",
        "\n",
        "    # Maskeye göre filtreleme\n",
        "    trainY = trainY.ravel()\n",
        "    trainPredict = trainPredict.ravel()\n",
        "    filtered_trainY = trainY[valid_mask_train]\n",
        "    filtered_trainPredict = trainPredict[valid_mask_train]\n",
        "\n",
        "    trainMAE = mean_absolute_error(filtered_trainY, filtered_trainPredict)\n",
        "    trainMSE = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
        "    trainmsMAPE = modified_smape(trainY, trainPredict)\n",
        "    print(\"train_MAE:\", trainMAE)\n",
        "    print(\"train_MSE:\", trainMSE)\n",
        "    print(\"train_modifiedsMAPE:\", trainmsMAPE)\n",
        "\n",
        "\n",
        "    # Orijinal test verisi (eksik değer içeren)\n",
        "    #orig_test = alldata[train_size+1:]  # Test kısmının orijinali\n",
        "    #orig_mask = ~np.isnan(orig_test)  # Eksik olmayanlar için maske\n",
        "    #valid_mask = orig_mask[look_back:]  # look_back sonrası geçerli indeksler\n",
        "\n",
        "    orig_test = series[split_idx+look_back:]  # Güncellendi\n",
        "    orig_mask = ~np.isnan(orig_test)\n",
        "    valid_mask = orig_mask  # look_back sonrası eksik olmayan veriler\n",
        "\n",
        "    print(\"TestPredict shape:\", testPredict.shape)\n",
        "    testY = testY.flatten()\n",
        "    testPredict = testPredict.flatten()\n",
        "\n",
        "    # Maskeye göre filtreleme\n",
        "    filtered_testY = testY[valid_mask]\n",
        "    filtered_testPredict = testPredict[valid_mask]\n",
        "\n",
        "    print(\"Filtered TestY shape:\", filtered_testY.shape)\n",
        "    print(\"Filtered TestPredict shape:\", filtered_testPredict.shape)\n",
        "\n",
        "    if len(filtered_testY) == 0:\n",
        "        print(\"Hata: Filtrelenmiş test verileri boş.\")\n",
        "    else:\n",
        "        testMAE = mean_absolute_error(filtered_testY, filtered_testPredict)\n",
        "        testMSE = np.sqrt(mean_squared_error(filtered_testY, filtered_testPredict))\n",
        "        testmsMAPE = modified_smape(filtered_testY, filtered_testPredict)\n",
        "        print(\"test_MAE:\", testMAE)\n",
        "        print(\"test_MSE:\", testMSE)\n",
        "        print(\"test_modifiedsMAPE:\", testmsMAPE)\n",
        "\n",
        "    '''\n",
        "    # === 8. Grafik\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(y_true_filtered, label=\"Gerçek\", linewidth=2)\n",
        "    plt.plot(y_pred_filtered, label=\"Tahmin\", linewidth=2, alpha=0.7)\n",
        "    plt.title(\"LSTM Test Tahmin Performansı\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    '''\n",
        "    summary_records.append({\n",
        "        \"Iteration\": run,\n",
        "        \"Imputation Time\": imputation_time,\n",
        "        \"Train MAE\": trainMAE,\n",
        "        \"Train RMSE\": trainMSE,\n",
        "        \"Train msMAPE\": trainmsMAPE,\n",
        "        \"Test MAE\": testMAE,\n",
        "        \"Test RMSE\": testMSE,\n",
        "        \"Test msMAPE\": testmsMAPE\n",
        "    })\n",
        "\n",
        "    # Tahmin değerlerini sakla (her iterasyon için ayrı sütun)\n",
        "    prediction_dict[f\"y_true_{run}\"] = filtered_testY\n",
        "    prediction_dict[f\"y_pred_{run}\"] = filtered_testPredict\n",
        "\n",
        "# ——— SONUÇLARI KAYDETMEK İÇİN DATAFRAME OLUŞTUR ———\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "summary_df = summary_df.round({\n",
        "    \"Imputation Time\": 4,\n",
        "    \"Train MSE\": 6, \"Train MAE\": 6, \"Train RMSE\": 6, \"Train msMAPE\": 4,\n",
        "    \"Test MSE\": 6, \"Test MAE\": 6, \"Test RMSE\": 6, \"Test msMAPE\": 4\n",
        "})\n",
        "summary_df.to_excel(\"Metin_sonmez_imputation_lstm_summary.xlsx\", index=False)\n",
        "\n",
        "\n",
        "# Tüm tahminleri hizalamak için en kısa uzunluğu belirle\n",
        "min_len = min(len(v) for v in prediction_dict.values())\n",
        "prediction_df = pd.DataFrame({k: v[:min_len] for k, v in prediction_dict.items()})\n",
        "\n",
        "print(prediction_df.head())\n",
        "# Excel’e kaydet (çalıştırmak istersen aşağıdaki yorumları kaldır)\n",
        "# summary_df.to_excel(\"imputation_lstm_summary.xlsx\", index=False)\n",
        "prediction_df.to_excel(\"NewOdunpazari_imputation_lstm_predictions.xlsx\", index=False)\n",
        "\n",
        "print(\"✅ 10 çalıştırmalık özet ve tahmin sonuçları hazır.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQUiWafc6xUM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import time\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def find_best_arima(train_data, validation_size=0.2, p_range=(0, 3), d_range=(0, 2), q_range=(0, 3), metric=\"rmse\"):\n",
        "    \"\"\"\n",
        "    Train verisi için en iyi (p, d, q) ARIMA parametrelerini seçer.\n",
        "\n",
        "    - Eğer metric=\"rmse\" ise, validation için train_data'yı ikiye böler ve bölünen train set ile modeli eğitir, doğrulama seti üzerinde RMSE hesaplar.\n",
        "    - Eğer metric=\"aic\" ise, tüm train_data ile modeli eğitir ve AIC'yi hesaplar.\n",
        "    \"\"\"\n",
        "\n",
        "    best_score = float(\"inf\")  # En iyi RMSE veya AIC değerini saklar\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # RMSE için train seti ayırma (validation kullanılır)\n",
        "    if metric == \"rmse\":\n",
        "        split_index = int(len(train_data) * (1 - validation_size))\n",
        "        train_set, val_set = train_data[:split_index], train_data[split_index:]\n",
        "    elif metric==\"aic\":\n",
        "        train_set = train_data  # AIC için tüm veri kullanılır\n",
        "        val_set = None  # AIC için validation set yok\n",
        "\n",
        "    # Grid Search için tüm (p, d, q) kombinasyonlarını oluştur\n",
        "    pdq_combinations = list(itertools.product(range(*p_range), range(*d_range), range(*q_range)))\n",
        "\n",
        "    for pdq in pdq_combinations:\n",
        "        try:\n",
        "            if metric == \"rmse\":\n",
        "                model = ARIMA(train_set, order=pdq)  # RMSE için sadece train_set ile eğit\n",
        "            else:\n",
        "                model = ARIMA(train_data, order=pdq)  # AIC için tüm train_data kullanılır\n",
        "\n",
        "            model_fit = model.fit()\n",
        "\n",
        "            # RMSE ile değerlendirme\n",
        "            if metric == \"rmse\":\n",
        "                predictions = model_fit.forecast(steps=len(val_set))\n",
        "                rmse = np.sqrt(mean_squared_error(val_set, predictions))\n",
        "                score = rmse\n",
        "                print(f\"ARIMA{pdq} - RMSE: {rmse}\")\n",
        "\n",
        "            # AIC ile değerlendirme\n",
        "            elif metric == \"aic\":\n",
        "                aic = model_fit.aic\n",
        "                score = aic\n",
        "                print(f\"ARIMA{pdq} - AIC: {aic}\")\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"metric parametresi 'rmse' veya 'aic' olmalıdır.\")\n",
        "\n",
        "            # En iyi modeli güncelle\n",
        "            if score < best_score:\n",
        "                best_score, best_params = score, pdq\n",
        "                best_model = model_fit\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ARIMA{pdq} başarısız: {e}\")\n",
        "            continue\n",
        "\n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uroozQSFk2_N"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import numpy as np\n",
        "def svr_grid_search(X,y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "      # Hiperparametre arama aralıkları\n",
        "    param_grid = {\n",
        "    'C': [1, 10, 100],  # Çok küçük ve çok büyük C değerlerini çıkardık\n",
        "    'epsilon': [0.01, 0.1],  # Yüksek epsilon değerleri genellikle tahmin doğruluğunu düşürür\n",
        "    'kernel': ['rbf'],  # Sadece RBF kernel kullanıyoruz\n",
        "    'gamma': ['scale', 0.1]  # Sadece \"scale\" ve makul bir sabit değer kullanıyoruz\n",
        "    }\n",
        "\n",
        "    # SVR Modeli\n",
        "    svr = SVR()\n",
        "\n",
        "    # Grid Search CV\n",
        "    grid_search = GridSearchCV(svr, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # En iyi model ve hiperparametreler\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    print(\"En iyi parametreler:\", best_params)\n",
        "    return  best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6vE8Ucd6gnM"
      },
      "outputs": [],
      "source": [
        "#STACKING+ARIMA\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "summary_records = []\n",
        "prediction_dict = {}\n",
        "\n",
        "for run in range(1, 11):\n",
        "    print(f\"▶️ Iteration {run} başlıyor...\")\n",
        "    dataset_path = '/content/drive/MyDrive/eksik_veri_simulasyonlari/Tepebasi_missing_with_existing_10pct.xlsx'\n",
        "    #dataset_path = '/content/drive/MyDrive/VNS_dataset/VNS_Cumhuriyet_Bulvari.xlsx'\n",
        "    df = pd.read_excel(dataset_path, parse_dates=['Tarih'])\n",
        "    df.replace(\"-\", np.nan, inplace=True)\n",
        "    #df = df.iloc[35049:]\n",
        "    print(df.head())\n",
        "    df['Tarih'] = pd.to_datetime(df['Tarih'], dayfirst=True, errors='coerce')\n",
        "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].apply(lambda x: np.nan if x < 0 else x)\n",
        "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].clip(lower=0, upper=np.nanmax(df['PM10 ( µg/m3 )']))\n",
        "\n",
        "    series = df['PM10 ( µg/m3 )']\n",
        "    dates = df['Tarih']\n",
        "    #series = series.iloc[18630:].reset_index(drop=True)\n",
        "    #dates = dates.iloc[18630:].reset_index(drop=True)\n",
        "    print(dates.head())\n",
        "    n_total = len(series)\n",
        "    split_idx = int(0.8 * n_total)\n",
        "    train_series = series[:split_idx].reset_index(drop=True)\n",
        "    test_series = series[split_idx:].reset_index(drop=True)\n",
        "    # Eğitim (Train) ve Test bölümleri\n",
        "    train_data = series[:split_idx]  # İlk %80\n",
        "    test_data =series[split_idx:]   # Son %20\n",
        "    orig_mask = ~np.isnan(test_data)\n",
        "    orig_mask_train = ~np.isnan(train_data)\n",
        "    # === 1. IMPUTATION YÖNTEMLERİ ===\n",
        "\n",
        "    def kalman_filter_imputation(train, test):\n",
        "        kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1,\n",
        "                          initial_state_covariance=1, transition_matrices=[1],\n",
        "                          observation_matrices=[1], transition_covariance=1,\n",
        "                          observation_covariance=1)\n",
        "        train_masked = np.ma.array(train, mask=np.isnan(train))\n",
        "        train_imputed, _ = kf.smooth(train_masked)\n",
        "\n",
        "        combined = np.concatenate([train_imputed.flatten(), test])\n",
        "        combined_masked = np.ma.array(combined, mask=np.isnan(combined))\n",
        "        combined_imputed, _ = kf.smooth(combined_masked)\n",
        "        test_imputed = combined_imputed[len(train):]\n",
        "        return train_imputed.flatten(), test_imputed.flatten()\n",
        "\n",
        "    def interpolation_imputation(train, test):\n",
        "        train_imputed = pd.Series(train).interpolate(limit_direction='both').to_numpy()\n",
        "        combined = np.concatenate([train_imputed, test])\n",
        "        combined_imputed = pd.Series(combined).interpolate(limit_direction='both').to_numpy()\n",
        "        return train_imputed, combined_imputed[len(train):]\n",
        "\n",
        "    def backward_fill_imputation(train, test):\n",
        "        return pd.Series(train).bfill().ffill().to_numpy(), pd.Series(test).bfill().ffill().to_numpy()\n",
        "\n",
        "    def svr_imputation_separate(train, test, n_lags=3):\n",
        "        def create_lagged_df(series):\n",
        "            df = pd.DataFrame({'y': series})\n",
        "            for i in range(1, n_lags + 1):\n",
        "                df[f'lag_{i}'] = df['y'].shift(i)\n",
        "            return df\n",
        "\n",
        "        feature_names = [f'lag_{i}' for i in range(1, n_lags + 1)]\n",
        "        train_df = create_lagged_df(pd.Series(train)).dropna()\n",
        "        X_train = train_df[feature_names].to_numpy()\n",
        "        y_train = train_df['y'].to_numpy()\n",
        "        svr = SVR().fit(X_train, y_train)\n",
        "\n",
        "        train_filled = np.copy(train)\n",
        "        for idx in np.where(np.isnan(train))[0]:\n",
        "            if idx >= n_lags and not np.isnan(train[idx - n_lags:idx]).any():\n",
        "                x_pred = train[idx - n_lags:idx][::-1].reshape(1, -1)\n",
        "                train_filled[idx] = svr.predict(x_pred)[0]\n",
        "        train_filled = pd.Series(train_filled).interpolate().bfill().ffill().to_numpy()\n",
        "\n",
        "        test_filled = np.copy(test)\n",
        "        for idx in np.where(np.isnan(test))[0]:\n",
        "            full_input = np.concatenate([train_filled[-n_lags:], test[:idx]])\n",
        "            if len(full_input) >= n_lags and not np.isnan(full_input[-n_lags:]).any():\n",
        "                x_pred = full_input[-n_lags:][::-1].reshape(1, -1)\n",
        "                test_filled[idx] = svr.predict(x_pred)[0]\n",
        "        test_filled = pd.Series(test_filled).interpolate().bfill().ffill().to_numpy()\n",
        "        return train_filled, test_filled\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    # === 2. TÜM YÖNTEMLERİ UYGULA ===\n",
        "    kalman_train, kalman_test = kalman_filter_imputation(train_series.to_numpy(), test_series.to_numpy())\n",
        "    linear_train, linear_test = interpolation_imputation(train_series, test_series)\n",
        "    bfill_train, bfill_test = backward_fill_imputation(train_series, test_series)\n",
        "    svr_train, svr_test = svr_imputation_separate(train_series.to_numpy(), test_series.to_numpy())\n",
        "\n",
        "    # === 3. STACKING VERİSİ (TÜM DOLU GÖZLEMLER) ===\n",
        "    non_nan_train_idx = np.where(~np.isnan(train_series))[0]\n",
        "\n",
        "    X_train_stack = np.column_stack([\n",
        "        kalman_train[non_nan_train_idx],\n",
        "        linear_train[non_nan_train_idx],\n",
        "        bfill_train[non_nan_train_idx],\n",
        "        svr_train[non_nan_train_idx]\n",
        "    ])\n",
        "    y_train_stack = train_series.to_numpy()[non_nan_train_idx]\n",
        "\n",
        "    nan_train_idx = np.where(np.isnan(train_series))[0]\n",
        "    nan_test_idx = np.where(np.isnan(test_series))[0]\n",
        "\n",
        "    X_test_stack = np.column_stack([\n",
        "        kalman_test[nan_test_idx],\n",
        "        linear_test[nan_test_idx],\n",
        "        bfill_test[nan_test_idx],\n",
        "        svr_test[nan_test_idx]\n",
        "    ])\n",
        "\n",
        "    # === 4. META MODEL: GRADIENT BOOSTING ===\n",
        "    if len(X_train_stack) >= 10:\n",
        "        meta_model = GradientBoostingRegressor(alpha=0.8,max_depth=4)\n",
        "        meta_model.fit(X_train_stack, y_train_stack)\n",
        "\n",
        "        # Yalnızca eksik değerler doldurulacak\n",
        "        train_series.iloc[nan_train_idx] = meta_model.predict(\n",
        "            np.column_stack([\n",
        "                kalman_train[nan_train_idx],\n",
        "                linear_train[nan_train_idx],\n",
        "                bfill_train[nan_train_idx],\n",
        "                svr_train[nan_train_idx]\n",
        "            ])\n",
        "        )\n",
        "        test_series.iloc[nan_test_idx] = meta_model.predict(X_test_stack)\n",
        "    else:\n",
        "        print(\"⚠️ Yetersiz eğitim verisi. Kalman ile dolduruluyor.\")\n",
        "        train_series.iloc[nan_train_idx] = kalman_train[nan_train_idx]\n",
        "        test_series.iloc[nan_test_idx] = kalman_test[nan_test_idx]\n",
        "    print(\"doldurmasonrasri:\", len(test_series))\n",
        "\n",
        "    filled_series = pd.concat([train_series, test_series], ignore_index=True)\n",
        "    end_time = time.time()\n",
        "    imputation_time = end_time - start_time\n",
        "    print(f\"Imputation Time: {imputation_time} seconds\")\n",
        "\n",
        "    # === 5. GÖRSELLEŞTİRME ===\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(dates, series, label='Orijinal Seri (Eksik)', alpha=0.6)\n",
        "    plt.plot(dates, filled_series, label='Stacking + GB Regressor ile Doldurulmuş Seri')\n",
        "    plt.xlabel('Tarih')\n",
        "    plt.ylabel('PM 2.5 ( µg/m3 )')\n",
        "    plt.title('Eksik Veri Doldurma: Stacking ve GB Regressor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Eksik değer sayısı (doldurma sonrası):\", filled_series.isnull().sum())\n",
        "    filled_only = filled_series\n",
        "    #filled_only = final_filled_series\n",
        "    print(len(filled_only))\n",
        "\n",
        "    # === 1. Train / Test böl\n",
        "    split_ratio = 0.8\n",
        "    split_idx = int(len(filled_only) * split_ratio)\n",
        "    train_series = filled_only[:split_idx]\n",
        "    test_series = filled_only[split_idx:]\n",
        "    print(len(train_series))\n",
        "    print(\"doldurmasonrasi bolme:\", len(test_series))\n",
        "    imputed_train = train_series\n",
        "    imputed_test = test_series\n",
        "\n",
        "\n",
        "    best_params= find_best_arima(imputed_train)\n",
        "    best_model = ARIMA(imputed_train, order=best_params)\n",
        "    training_time_start=time.time()\n",
        "    best_model_fit=best_model.fit()\n",
        "\n",
        "    training_time_end=time.time()\n",
        "    training_time=training_time_end-training_time_start\n",
        "    test_start = time.time()\n",
        "    testPredict = best_model_fit.forecast(steps=len(imputed_test))\n",
        "    test_end = time.time()\n",
        "    trainPredict = best_model_fit.predict(start=0, end=len(imputed_train)-1)\n",
        "    print(\"tahmin\", len(testPredict))\n",
        "\n",
        "    filtered_trainY = imputed_train.reset_index(drop=True)[orig_mask_train.reset_index(drop=True)]\n",
        "    filtered_trainPredict = pd.Series(trainPredict).reset_index(drop=True)[orig_mask_train.reset_index(drop=True)]\n",
        "\n",
        "    #filtered_trainY = imputed_train[orig_mask_train]\n",
        "    #filtered_trainPredict = trainPredict[orig_mask_train]\n",
        "\n",
        "    filtered_testY = imputed_test.reset_index(drop=True)[orig_mask.reset_index(drop=True)]\n",
        "    filtered_testPredict = pd.Series(testPredict).reset_index(drop=True)[orig_mask.reset_index(drop=True)]\n",
        "    # Maskeye göre filtreleme\n",
        "    #filtered_testY = imputed_test[orig_mask]\n",
        "    #filtered_testPredict = testPredict[orig_mask]\n",
        "\n",
        "    # === 7. Performans metrikleri\n",
        "    def modified_smape(y_true, y_pred):\n",
        "        return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "    # === Performans metrikleri\n",
        "\n",
        "    train_mse = mean_squared_error(filtered_trainY, filtered_trainPredict)\n",
        "    train_mae = mean_absolute_error(filtered_trainY, filtered_trainPredict)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    train_msmape = modified_smape(filtered_trainY, filtered_trainPredict)\n",
        "    print(train_mse, train_mae, train_rmse, train_msmape) # yüzde olarak ifade etmek için *100\n",
        "\n",
        "    msmape = modified_smape(filtered_testY, filtered_testPredict)\n",
        "    mse = mean_squared_error(filtered_testY, filtered_testPredict)\n",
        "    mae = mean_absolute_error(filtered_testY, filtered_testPredict)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "\n",
        "    print(mse, mae, rmse, msmape)\n",
        "    print(filtered_testPredict.isna().sum())\n",
        "    print(filtered_testY.isna().sum())\n",
        "    summary_records.append({\n",
        "        \"Iteration\": run,\n",
        "        \"Imputation Time\": imputation_time,\n",
        "        \"Train MSE\": train_mse,\n",
        "        \"Train MAE\": train_mae,\n",
        "        \"Train RMSE\": train_rmse,\n",
        "        \"Train msMAPE\": train_msmape,\n",
        "        \"Test MSE\": mse,\n",
        "        \"Test MAE\": mae,\n",
        "        \"Test RMSE\": rmse,\n",
        "        \"Test msMAPE\": msmape\n",
        "    })\n",
        "\n",
        "\n",
        "# ——— SONUÇLARI KAYDETMEK İÇİN DATAFRAME OLUŞTUR ———\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "summary_df = summary_df.round({\n",
        "    \"Imputation Time\": 4,\n",
        "    \"Train MSE\": 6, \"Train MAE\": 6, \"Train RMSE\": 6, \"Train msMAPE\": 4,\n",
        "    \"Test MSE\": 6, \"Test MAE\": 6, \"Test RMSE\": 6, \"Test msMAPE\": 4\n",
        "})\n",
        "summary_df.to_excel(\"Tepe10.xlsx\", index=False)\n",
        "\n",
        "print(\"✅ 10 çalıştırmalık özet ve tahmin sonuçları hazır.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVttNan3vbR0"
      },
      "outputs": [],
      "source": [
        "def create_lagged_features(series, look_back):\n",
        "    X, y = [], []\n",
        "    series = np.array(series)\n",
        "    for i in range(len(series) - look_back):\n",
        "        X.append(series[i:i + look_back])\n",
        "        y.append(series[i + look_back])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v8Vdaf4kLWT"
      },
      "outputs": [],
      "source": [
        "#SVR+STACKING\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "summary_records = []\n",
        "prediction_dict = {}\n",
        "\n",
        "for run in range(1, 11):\n",
        "    print(f\"▶️ Iteration {run} başlıyor...\")\n",
        "    #dataset_path = '/content/drive/MyDrive/eksik_veri_simulasyonlari/MS_missing_with_existing_20pct.xlsx'\n",
        "    dataset_path = '/content/drive/MyDrive/VNS_dataset/VNS_Tepebasi.xlsx'\n",
        "    df = pd.read_excel(dataset_path, parse_dates=['Tarih'])\n",
        "    df.replace(\"-\", np.nan, inplace=True)\n",
        "    df = df.iloc[18630:]\n",
        "    print(df.head())\n",
        "    df['Tarih'] = pd.to_datetime(df['Tarih'], dayfirst=True, errors='coerce')\n",
        "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].apply(lambda x: np.nan if x < 0 else x)\n",
        "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].clip(lower=0, upper=np.nanmax(df['PM10 ( µg/m3 )']))\n",
        "\n",
        "    series = df['PM10 ( µg/m3 )']\n",
        "    dates = df['Tarih']\n",
        "    #series = series.iloc[18630:].reset_index(drop=True)\n",
        "    #dates = dates.iloc[18630:].reset_index(drop=True)\n",
        "    print(dates.head())\n",
        "    n_total = len(series)\n",
        "    original_nan_mask = series.isna().to_numpy()\n",
        "    split_idx = int(0.8 * n_total)\n",
        "    train_series = series[:split_idx].reset_index(drop=True)\n",
        "    test_series = series[split_idx:].reset_index(drop=True)\n",
        "\n",
        "    # === 1. IMPUTATION YÖNTEMLERİ ===\n",
        "    def modified_smape(y_true, y_pred):\n",
        "      return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "    def kalman_filter_imputation(train, test):\n",
        "        kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1,\n",
        "                          initial_state_covariance=1, transition_matrices=[1],\n",
        "                          observation_matrices=[1], transition_covariance=1,\n",
        "                          observation_covariance=1)\n",
        "        train_masked = np.ma.array(train, mask=np.isnan(train))\n",
        "        train_imputed, _ = kf.smooth(train_masked)\n",
        "\n",
        "        combined = np.concatenate([train_imputed.flatten(), test])\n",
        "        combined_masked = np.ma.array(combined, mask=np.isnan(combined))\n",
        "        combined_imputed, _ = kf.smooth(combined_masked)\n",
        "        test_imputed = combined_imputed[len(train):]\n",
        "        return train_imputed.flatten(), test_imputed.flatten()\n",
        "\n",
        "    def interpolation_imputation(train, test):\n",
        "        train_imputed = pd.Series(train).interpolate(limit_direction='both').to_numpy()\n",
        "        combined = np.concatenate([train_imputed, test])\n",
        "        combined_imputed = pd.Series(combined).interpolate(limit_direction='both').to_numpy()\n",
        "        return train_imputed, combined_imputed[len(train):]\n",
        "\n",
        "    def backward_fill_imputation(train, test):\n",
        "        return pd.Series(train).bfill().ffill().to_numpy(), pd.Series(test).bfill().ffill().to_numpy()\n",
        "\n",
        "    def svr_imputation_separate(train, test, n_lags=3):\n",
        "        def create_lagged_df(series):\n",
        "            df = pd.DataFrame({'y': series})\n",
        "            for i in range(1, n_lags + 1):\n",
        "                df[f'lag_{i}'] = df['y'].shift(i)\n",
        "            return df\n",
        "\n",
        "        feature_names = [f'lag_{i}' for i in range(1, n_lags + 1)]\n",
        "        train_df = create_lagged_df(pd.Series(train)).dropna()\n",
        "        X_train = train_df[feature_names].to_numpy()\n",
        "        y_train = train_df['y'].to_numpy()\n",
        "        svr = SVR().fit(X_train, y_train)\n",
        "\n",
        "        train_filled = np.copy(train)\n",
        "        for idx in np.where(np.isnan(train))[0]:\n",
        "            if idx >= n_lags and not np.isnan(train[idx - n_lags:idx]).any():\n",
        "                x_pred = train[idx - n_lags:idx][::-1].reshape(1, -1)\n",
        "                train_filled[idx] = svr.predict(x_pred)[0]\n",
        "        train_filled = pd.Series(train_filled).interpolate().bfill().ffill().to_numpy()\n",
        "\n",
        "        test_filled = np.copy(test)\n",
        "        for idx in np.where(np.isnan(test))[0]:\n",
        "            full_input = np.concatenate([train_filled[-n_lags:], test[:idx]])\n",
        "            if len(full_input) >= n_lags and not np.isnan(full_input[-n_lags:]).any():\n",
        "                x_pred = full_input[-n_lags:][::-1].reshape(1, -1)\n",
        "                test_filled[idx] = svr.predict(x_pred)[0]\n",
        "        test_filled = pd.Series(test_filled).interpolate().bfill().ffill().to_numpy()\n",
        "        return train_filled, test_filled\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    # === 2. TÜM YÖNTEMLERİ UYGULA ===\n",
        "    kalman_train, kalman_test = kalman_filter_imputation(train_series.to_numpy(), test_series.to_numpy())\n",
        "    linear_train, linear_test = interpolation_imputation(train_series, test_series)\n",
        "    bfill_train, bfill_test = backward_fill_imputation(train_series, test_series)\n",
        "    svr_train, svr_test = svr_imputation_separate(train_series.to_numpy(), test_series.to_numpy())\n",
        "\n",
        "    # === 3. STACKING VERİSİ (TÜM DOLU GÖZLEMLER) ===\n",
        "    non_nan_train_idx = np.where(~np.isnan(train_series))[0]\n",
        "\n",
        "    X_train_stack = np.column_stack([\n",
        "        kalman_train[non_nan_train_idx],\n",
        "        linear_train[non_nan_train_idx],\n",
        "        bfill_train[non_nan_train_idx],\n",
        "        svr_train[non_nan_train_idx]\n",
        "    ])\n",
        "    y_train_stack = train_series.to_numpy()[non_nan_train_idx]\n",
        "\n",
        "    nan_train_idx = np.where(np.isnan(train_series))[0]\n",
        "    nan_test_idx = np.where(np.isnan(test_series))[0]\n",
        "\n",
        "    X_test_stack = np.column_stack([\n",
        "        kalman_test[nan_test_idx],\n",
        "        linear_test[nan_test_idx],\n",
        "        bfill_test[nan_test_idx],\n",
        "        svr_test[nan_test_idx]\n",
        "    ])\n",
        "\n",
        "    # === 4. META MODEL: GRADIENT BOOSTING ===\n",
        "    if len(X_train_stack) >= 10:\n",
        "        meta_model = GradientBoostingRegressor()\n",
        "        meta_model.fit(X_train_stack, y_train_stack)\n",
        "\n",
        "        # Yalnızca eksik değerler doldurulacak\n",
        "        train_series.iloc[nan_train_idx] = meta_model.predict(\n",
        "            np.column_stack([\n",
        "                kalman_train[nan_train_idx],\n",
        "                linear_train[nan_train_idx],\n",
        "                bfill_train[nan_train_idx],\n",
        "                svr_train[nan_train_idx]\n",
        "            ])\n",
        "        )\n",
        "        test_series.iloc[nan_test_idx] = meta_model.predict(X_test_stack)\n",
        "    else:\n",
        "        print(\"⚠️ Yetersiz eğitim verisi. Kalman ile dolduruluyor.\")\n",
        "        train_series.iloc[nan_train_idx] = kalman_train[nan_train_idx]\n",
        "        test_series.iloc[nan_test_idx] = kalman_test[nan_test_idx]\n",
        "\n",
        "    filled_series = pd.concat([train_series, test_series], ignore_index=True)\n",
        "    end_time = time.time()\n",
        "    imputation_time = end_time - start_time\n",
        "    print(f\"Imputation Time: {imputation_time} seconds\")\n",
        "\n",
        "    # === 5. GÖRSELLEŞTİRME ===\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(dates, series, label='Orijinal Seri (Eksik)', alpha=0.6)\n",
        "    plt.plot(dates, filled_series, label='Stacking + GB Regressor ile Doldurulmuş Seri')\n",
        "    plt.xlabel('Tarih')\n",
        "    plt.ylabel('PM 2.5 ( µg/m3 )')\n",
        "    plt.title('Eksik Veri Doldurma: Stacking ve GB Regressor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Eksik değer sayısı (doldurma sonrası):\", filled_series.isnull().sum())\n",
        "    filled_only = filled_series\n",
        "    #filled_only = final_filled_series\n",
        "    print(len(filled_only))\n",
        "\n",
        "    # === 1. Train / Test böl\n",
        "    split_ratio = 0.8\n",
        "    split_idx = int(len(filled_only) * split_ratio)\n",
        "    train_series = filled_only[:split_idx]\n",
        "    test_series = filled_only[split_idx:]\n",
        "    print(len(train_series))\n",
        "    print(len(test_series))\n",
        "\n",
        "    print(train_series)\n",
        "    print(test_series)\n",
        "    look_back = 24\n",
        "    X_train, y_train = create_lagged_features(train_series, look_back)\n",
        "    X_test, y_test = create_lagged_features(test_series, look_back)\n",
        "    scaler_X = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "    best_params=svr_grid_search(X_train_scaled,y_train_scaled)\n",
        "\n",
        "\n",
        "    best_svr = SVR(**best_params)\n",
        "    #best_svr=SVR()\n",
        "    training_time_start=time.time()\n",
        "    best_svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "    training_time_end=time.time()\n",
        "    training_time=training_time_end-training_time_start\n",
        "\n",
        "    # 📌 **7. Tahmin Yapma**\n",
        "    test_start = time.time()\n",
        "    y_pred_scaled = best_svr.predict(X_test_scaled)\n",
        "    test_end = time.time()\n",
        "    #testPredict = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    # missing value olmayanlardan üzerinden mterik hesaplarken lage göre düzenleme yapmak lazım\n",
        "    # 📌 **8. Model Performansını Değerlendirme**\n",
        "\n",
        "    testY = y_test  # y_test zaten mevcut\n",
        "    trainPredict = scaler_y.inverse_transform(best_svr.predict(X_train_scaled).reshape(-1, 1)).flatten()\n",
        "    trainY = y_train  # unscaled değer\n",
        "\n",
        "    testPredict = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    testY = y_test\n",
        "\n",
        "    orig_train = series[0:split_idx]\n",
        "    orig_mask_train = ~np.isnan(orig_train)\n",
        "    valid_mask_train = orig_mask_train[look_back:look_back + len(trainY)]\n",
        "\n",
        "    print(f\"valid_mask_train: {valid_mask_train.shape}\")\n",
        "    print(f\"trainY: {trainY.shape}, trainPredict: {trainPredict.shape}\")\n",
        "\n",
        "    # Maskeye göre filtreleme\n",
        "    trainY = trainY.ravel()\n",
        "    trainPredict = trainPredict.ravel()\n",
        "    filtered_trainY = trainY[valid_mask_train]\n",
        "    filtered_trainPredict = trainPredict[valid_mask_train]\n",
        "\n",
        "\n",
        "    trainMAE = mean_absolute_error(filtered_trainY, filtered_trainPredict)\n",
        "    trainMSE = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
        "    trainmsMAPE = modified_smape(trainY, trainPredict)\n",
        "    print(\"train_MAE:\", trainMAE)\n",
        "    print(\"train_MSE:\", trainMSE)\n",
        "    print(\"train_modifiedsMAPE:\", trainmsMAPE)\n",
        "\n",
        "\n",
        "    # Orijinal test verisi (eksik değer içeren)\n",
        "    #orig_test = alldata[train_size+1:]  # Test kısmının orijinali\n",
        "    #orig_mask = ~np.isnan(orig_test)  # Eksik olmayanlar için maske\n",
        "    #valid_mask = orig_mask[look_back:]  # look_back sonrası geçerli indeksler\n",
        "\n",
        "    orig_test = series[split_idx+look_back:]  # Güncellendi\n",
        "    orig_mask = ~np.isnan(orig_test)\n",
        "    valid_mask = orig_mask  # look_back sonrası eksik olmayan veriler\n",
        "\n",
        "    print(\"TestPredict shape:\", testPredict.shape)\n",
        "    testY = testY.flatten()\n",
        "    testPredict = testPredict.flatten()\n",
        "\n",
        "    # Maskeye göre filtreleme\n",
        "    filtered_testY = testY[valid_mask]\n",
        "    filtered_testPredict = testPredict[valid_mask]\n",
        "\n",
        "    print(\"Filtered TestY shape:\", filtered_testY.shape)\n",
        "    print(\"Filtered TestPredict shape:\", filtered_testPredict.shape)\n",
        "\n",
        "    if len(filtered_testY) == 0:\n",
        "        print(\"Hata: Filtrelenmiş test verileri boş.\")\n",
        "    else:\n",
        "        testMAE = mean_absolute_error(filtered_testY, filtered_testPredict)\n",
        "        testMSE = np.sqrt(mean_squared_error(filtered_testY, filtered_testPredict))\n",
        "        testmsMAPE = modified_smape(filtered_testY, filtered_testPredict)\n",
        "        print(\"test_MAE:\", testMAE)\n",
        "        print(\"test_MSE:\", testMSE)\n",
        "        print(\"test_modifiedsMAPE:\", testmsMAPE)\n",
        "\n",
        "    '''\n",
        "    # === 8. Grafik\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(y_true_filtered, label=\"Gerçek\", linewidth=2)\n",
        "    plt.plot(y_pred_filtered, label=\"Tahmin\", linewidth=2, alpha=0.7)\n",
        "    plt.title(\"LSTM Test Tahmin Performansı\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    '''\n",
        "    summary_records.append({\n",
        "        \"Iteration\": run,\n",
        "        \"Imputation Time\": imputation_time,\n",
        "        \"Train MAE\": trainMAE,\n",
        "        \"Train RMSE\": trainMSE,\n",
        "        \"Train msMAPE\": trainmsMAPE,\n",
        "        \"Test MAE\": testMAE,\n",
        "        \"Test RMSE\": testMSE,\n",
        "        \"Test msMAPE\": testmsMAPE\n",
        "    })\n",
        "\n",
        "    # Tahmin değerlerini sakla (her iterasyon için ayrı sütun)\n",
        "    prediction_dict[f\"y_true_{run}\"] = filtered_testY\n",
        "    prediction_dict[f\"y_pred_{run}\"] = filtered_testPredict\n",
        "\n",
        "# ——— SONUÇLARI KAYDETMEK İÇİN DATAFRAME OLUŞTUR ———\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "summary_df = summary_df.round({\n",
        "    \"Imputation Time\": 4,\n",
        "    \"Train MSE\": 6, \"Train MAE\": 6, \"Train RMSE\": 6, \"Train msMAPE\": 4,\n",
        "    \"Test MSE\": 6, \"Test MAE\": 6, \"Test RMSE\": 6, \"Test msMAPE\": 4\n",
        "})\n",
        "summary_df.to_excel(\"MS20_imputation_svr_summary.xlsx\", index=False)\n",
        "\n",
        "\n",
        "# Tüm tahminleri hizalamak için en kısa uzunluğu belirle\n",
        "min_len = min(len(v) for v in prediction_dict.values())\n",
        "prediction_df = pd.DataFrame({k: v[:min_len] for k, v in prediction_dict.items()})\n",
        "\n",
        "print(prediction_df.head())\n",
        "# Excel’e kaydet (çalıştırmak istersen aşağıdaki yorumları kaldır)\n",
        "# summary_df.to_excel(\"imputation_lstm_summary.xlsx\", index=False)\n",
        "prediction_df.to_excel(\"MS20_imputation_svr_predictions.xlsx\", index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}